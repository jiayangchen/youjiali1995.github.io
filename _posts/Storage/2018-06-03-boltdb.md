---
title: boltdb 源码分析
layout: post
excerpt: 介绍 boltdb 的设计与实现。
categories: Storage
---

{% include toc %}

[boltdb](https://github.com/boltdb/bolt) 是 `go` 语言实现的嵌入式 `K/V` 数据库，核心代码在4000行左右，容易学习。

## 简介
`boltdb` 有如下性质：
* `K/V` 型存储，使用 `B+` 树索引。
* 支持 `namespace`，每对 `K/V` 存放在一个 `Bucket` 下，不同 `Bucket` 可以有相同的 `key`，支持嵌套的 `Bucket`。
* 支持事务(`ACID`)，使用 `MVCC`，允许多个读事务和一个写事务并发执行，但是读事务有可能会阻塞写事务，适合读多写少的场景。

下面这段代码实现了最基本的功能：
1. 打开文件 `my.db` 对应的数据库；
2. 开始一个写事务
3. 在该事务中创建 `Bucket`: `MyBucket`；
4. 在 `MyBucket` 中写入 `key: foo, value: bar`；
5. 提交事务。

```go
// Open the my.db data file in your current directory.
// It will be created if it doesn't exist.
db, err := bolt.Open("my.db", 0600, nil)
if err != nil {
    log.Fatal(err)
}
defer db.Close()

// Start a writable transaction.
tx, err := db.Begin(true)
if err != nil {
    return err
}
defer tx.Rollback()

// Use the transaction...
bucket, err := tx.CreateBucket([]byte("MyBucket"))
if err != nil {
    return err
}
bucket.Put([]byte("foo"), []byte("bar"))

// Commit the transaction and check for error.
if err := tx.Commit(); err != nil {
    return err
}
```

## 存储
概括来讲，`boltdb` 的存储有如下特点：
* 每个 `db` 对应一个文件，文件按照 `page size`(一般为 `4096 Bytes`) 划分为 `page`:
    * 前2个 `page` 保存 `metadata`；
    * 特殊的 `page` 保存 `freelist`，存放空闲 `page` 的 `id`；
    * 剩下的 `page` 构成一个 `B+` 树结构。
* 每个 `Bucket` 是一个完整的 `B+` 树；
* `B+` 树的每个结点对应一个或多个连续的 `page`；
* 因为内存比磁盘小，一般会实现 `page cache` 缓存部分 `page`，比如使用 `LRU` 算法。`boltdb` 没有实现，而是使用 `mmap()` 创建共享、只读的文件映射并调用 `madvise(MADV_RANDOM)`，由操作系统
管理 `page cache`；
* 没有 `WAL`，事务中所有操作都在内存中进行，只有 `commit` 时才会写到磁盘；
* `commit` 时会将 `dirty page` 写入新的 `page`，从而保证同时读的事务不受到影响。

### B+ 树索引
`boltdb` 中典型的 `B+` 树结构如下：

![image](/assets/images/boltdb/B+tree.png)

它的实现和通常意义上的 `B+` 树有些不同，结点上的 `key` 和 `val` 个数是相同的，而一般 `B+` 树 `val` 会比 `key` 多一个:
* `branch`: 每对 `key/val` 指向一个子节点，`key` 是子节点的起始 `range`，`val` 存放子节点的 `page id`；
* `leaf`: 每对 `key/val` 存放数据，没有指针指向 `sibiling node`；通常的 `B+` 树多出的一个指针会指向 `sibiling node`。

`boltdb` 中有 3 个结构和 `B+` 树密切相关：
* `page`: 大小一般为 `4096 bytes`，对应文件里的每个 `page`，读写文件都是以 `page` 为单位。
* `node`: `B+` 树的单个结点，访问结点时首先将 `page` 的内容转换为内存中的 `node`，每个 `node` 对应一个或多个连续的 `page`。
* `Bucket`: 每个 `Bucket` 都是一个完整的 `B+` 树，所有操作都是针对 `Bucket`。

一个典型的查找过程如下：
1. 首先找到 `Bucket` 的根节点，也就是 `B+` 树的根节点的 `page id`；
2. 读取对应的 `page`，转化为内存中的 `node`；
3. 若是 `branch node`，则根据 `key` 查找合适的子节点的 `page id`;
4. 重复2、3直到找到 `leaf node`，返回 `node` 中对应的 `val`。

### page
文件被组织为 `page size(4096 bytes)` 的 `page`，对应的结构如下：
```go
type page struct {
	id    pgid // page id
	flags uint16 // 区分不同类型的 page
	count    uint16 // page data 中的数据个数
	overflow uint32 // 若单个 page 大小不够，会分配多个 page
	ptr uintptr // 存放 page data 的起始地址
}
```
`ptr` 是保存数据的起始地址，不同类型 `page` 保存的数据格式也不同，共有4种 `page`, 通过 `flags` 区分:
* `meta page`: 存放 `db` 的 `meta data`。
* `freelist page`: 存放 `db` 的空闲 `page`。
* `branch page`: 存放 `branch node` 的数据。
* `leaf page`: 存放 `leaf node` 的数据。


磁盘上的格式和结构体相同，将数据写入文件会首先分配一个 `page`，然后设置 `page header` 和 `page data`:
![images](/assets/images/boltdb/page.png)

`boltdb` 直接将 `page` 结构体的二进制格式写入文件，避免了序列化和反序列化的开销:
* 写的时候直接将需要写入的结构体转换为 `byte` 数组写入文件：
```go
ptr := (*[maxAllocSize]byte)(unsafe.Pointer(p))
```
* 读的时候直接将 `byte` 数组转换为对应结构体即可:
```go
// page retrieves a page reference from the mmap based on the current page size.
func (db *DB) page(id pgid) *page {
	pos := id * pgid(db.pageSize)
	return (*page)(unsafe.Pointer(&db.data[pos]))
}
```
* 需要注意只能转换为数组而不是 `slice`，否则会受到 `sliceHeader` 的影响。

#### page cache
`boltdb` 没有实现 `page cache`，而是调用 `mmap()` 将整个文件映射进来，并调用 `madvise(MADV_RANDOM)` 由操作系统管理 `page cache`，后续对磁盘上文件的所有读操作直接读取 `db.data` 即可，简化了实现:
```go
// mmap memory maps a DB's data file.
func mmap(db *DB, sz int) error {
	// Map the data file to memory.
	b, err := syscall.Mmap(int(db.file.Fd()), 0, sz, syscall.PROT_READ, syscall.MAP_SHARED|db.MmapFlags)
	if err != nil {
		return err
	}

	// Advise the kernel that the mmap is accessed randomly.
	if err := madvise(b, syscall.MADV_RANDOM); err != nil {
		return fmt.Errorf("madvise: %s", err)
	}

	// Save the original byte slice and convert to a byte array pointer.
	db.dataref = b
	db.data = (*[maxMapSize]byte)(unsafe.Pointer(&b[0]))
	db.datasz = sz
	return nil
}
```

### node
#### 文件格式
`leaf node` 在文件中的格式如下:
![image](/assets/images/boltdb/leafNode.png)

`node` 的数据存放在 `page.ptr` 的位置:
* 首先是所有的 `leafPageElement`:
```go
// leafPageElement represents a node on a leaf page.
type leafPageElement struct {
	flags uint32 // 通过 flags 区分 subbucket 和普通 value
	pos   uint32 // key 距离 leafPageElement 的位移
	ksize uint32
	vsize uint32
}
```
* 然后是所有的 `key` 和 `value`。
* 通过 `pos`、`ksize` 和 `vsize` 获取对应的 `key/value`：
    * `&leafPageElement + pos == &key`。
    * `&leafPageElement + pos + ksize == &val`。

`branch node` 在文件中的格式如下:
![image](/assets/images/boltdb/branchNode.png)

和 `leaf node` 的区别是：`branch node` 的 `value` 是子节点的 `page id`，存放在 `branchPageElement` 里，而 `key` 的存储相同都是通过 `pos` 得到：
```go
// branchPageElement represents a node on a branch page.
type branchPageElement struct {
	pos   uint32
	ksize uint32
	pgid  pgid
}
```

#### 内存中
访问结点时，首先要将 `page` 反序列化后得到得到 `node` 结构体，代表 `B+` 树中一个结点:
```go
// node represents an in-memory, deserialized page.
type node struct {
	bucket     *Bucket
	isLeaf     bool // 区分 branch node 和 leaf node
	unbalanced bool
	spilled    bool
	key        []byte // 该 node 的起始 key
	pgid       pgid
	parent     *node
	children nodes
	inodes   inodes // 存放 node 的数据
}
```

`inodes` 保存了该 `node` 的 `K/V` 数据:
```go
// inode represents an internal node inside of a node.
// It can be used to point to elements in a page or point
// to an element which hasn't been added to a page yet.
type inode struct {
	flags uint32 // 用于 leaf node，区分是正常 value 还是 subbucket
	pgid  pgid // 用于 branch node, 子节点的 page id
	key   []byte
	value []byte
}
```

`node` 和 `page` 的相互转换通过 `node.read(p *page)` 和 `node.write(p *page)`，具体实现不再赘述。

### Bucket
有了上面的铺垫，现在可以看一下 `get/put` 等操作的实现了。  

操作都是对 `Bucket` 进行操作，`Bucket` 是一个 `namespace`，相当于一张表，一个 `Bucket` 代表一个完整的 `B+` 树，结构如下：
```go
// Bucket represents a collection of key/value pairs inside the database.
type Bucket struct {
	*bucket
	tx      *Tx // the associated transaction
	buckets map[string]*Bucket // subbucket cache
	page *page // inline page reference
	rootNode *node // materialized node for the root page.
	nodes map[pgid]*node // node cache
	// Sets the threshold for filling nodes when they split. By default,
	// the bucket will fill to 50% but it can be useful to increase this
	// amount if you know that your write workloads are mostly append-only.
	//
	// This is non-persisted across transactions so it must be set in every Tx.
	FillPercent float64
}
// bucket represents the on-file representation of a bucket.
// This is stored as the "value" of a bucket key. If the bucket is small enough,
// then its root page can be stored inline in the "value", after the bucket
// header. In the case of inline buckets, the "root" will be 0.
type bucket struct {
	root pgid // page id of the bucket's root-level page
	sequence uint64 // monotonically incrementing, used by NextSequence()
}
```

假设我们现在获取了一个 `Bucket`，`get/put` 操作首先都要查找到 `key` 对应的位置，`boltdb` 通过 `Cursor` 实现:
```go
// Cursor creates a cursor associated with the bucket.
// The cursor is only valid as long as the transaction is open.
// Do not use a cursor after the transaction is closed.
func (b *Bucket) Cursor() *Cursor {
	// Update transaction statistics.
	b.tx.stats.CursorCount++

	// Allocate and return a cursor.
	return &Cursor{
		bucket: b,
		stack:  make([]elemRef, 0),
	}
}
type Cursor struct {
	bucket *Bucket
	stack  []elemRef
}
type elemRef struct {
	page  *page
	node  *node
	index int
}
```

首先从 `Bucket.root` 对应的 `page` 开始，递归的进行查找，直到 `leaf node`。`Cursor.stack` 中保存了查找对应 `key` 的路径，栈顶保存了 `key` 所在的结点和位置:
* `get` 操作返回栈顶结点中对应的 `key/val`:
```go
// keyValue returns the key and value of the current leaf element.
func (c *Cursor) keyValue() ([]byte, []byte, uint32) {
	ref := &c.stack[len(c.stack)-1]
	if ref.count() == 0 || ref.index >= ref.count() {
		return nil, nil, 0
	}

	// Retrieve value from node.
	if ref.node != nil {
		inode := &ref.node.inodes[ref.index]
		return inode.key, inode.value, inode.flags
	}

	// Or retrieve value from page.
	elem := ref.page.leafPageElement(uint16(ref.index))
	return elem.key(), elem.value(), elem.flags
}
```
* `put` 操作将对应的 `key/val` 添加到栈顶结点:
```go
c.node().put(key, key, value, 0, 0) // c.node() 返回栈顶对应的结点
// put inserts a key/value.
func (n *node) put(oldKey, newKey, value []byte, pgid pgid, flags uint32) {
	if pgid >= n.bucket.tx.meta.pgid {
		panic(fmt.Sprintf("pgid (%d) above high water mark (%d)", pgid, n.bucket.tx.meta.pgid))
	} else if len(oldKey) <= 0 {
		panic("put: zero-length old key")
	} else if len(newKey) <= 0 {
		panic("put: zero-length new key")
	}

	// Find insertion index.
	index := sort.Search(len(n.inodes), func(i int) bool { return bytes.Compare(n.inodes[i].key, oldKey) != -1 })

	// Add capacity and shift nodes if we don't have an exact match and need to insert.
	exact := (len(n.inodes) > 0 && index < len(n.inodes) && bytes.Equal(n.inodes[index].key, oldKey))
	if !exact {
		n.inodes = append(n.inodes, inode{})
		copy(n.inodes[index+1:], n.inodes[index:])
	}

	inode := &n.inodes[index]
	inode.flags = flags
	inode.key = newKey
	inode.value = value
	inode.pgid = pgid
	_assert(len(inode.key) > 0, "put: zero-length inode key")
}
```

#### subbucket
`boltdb` 支持嵌套的 `Bucket`，对于父 `Bucket` 而言，`subbucket` 只是特殊的 `value` 而已，设置 `leafPageElement.flags = bucketLeafFlag` 标记，而 `subbucket` 本身是一个完整的 `B+` 树:
![image](/assets/images/boltdb/subbucketTree.png)

`subbucket` 在父 `bucket` 的 `leaf node` 中存储格式如下:
* `root`: `subbucket` 的 `root` 结点的 `page id`。
* `sequence`: `boltdb` 支持每 `bucket` 的自增 `id`，即这里的 `sequence`。
```go
// bucket represents the on-file representation of a bucket.
// This is stored as the "value" of a bucket key. If the bucket is small enough,
// then its root page can be stored inline in the "value", after the bucket
// header. In the case of inline buckets, the "root" will be 0.
type bucket struct {
	root pgid // page id of the bucket's root-level page
	sequence uint64 // monotonically incrementing, used by NextSequence()
}
```

![image](/assets/images/boltdb/subbucketLeafNode.png)

为了实现的统一，`db` 有一个 `root Bucket`，也就是整个文件构成的 `B+` 树的 `root` 结点对应的 `Bucket`，之后创建的 `Bucket` 都是 `root Bucket` 的 `subbucket`。

#### inline bucket
从上面看到，父 `bucket` 中只保存了 `subbucket` 的 `bucket header`，每个 `subbucket` 都会占据至少一个 `page`，若 `subbucket` 中的数据很少，会造成磁盘空间的浪费。`inline bucket` 是将小的 `subbucket` 的
值完整放在父 `bucket` 的 `leaf node` 上，从而减少 `page` 的个数。`inline bucket` 的限制如下：
* 没有 `subbucket`；
* 整个 `Bucket` 的大小不能超过 `page size/4`。
```go
// inlineable returns true if a bucket is small enough to be written inline
// and if it contains no subbuckets. Otherwise returns false.
func (b *Bucket) inlineable() bool {
	var n = b.rootNode

	// Bucket must only contain a single leaf node.
	if n == nil || !n.isLeaf {
		return false
	}

	// Bucket is not inlineable if it contains subbuckets or if it goes beyond
	// our threshold for inline bucket size.
	var size = pageHeaderSize
	for _, inode := range n.inodes {
		size += leafPageElementSize + len(inode.key) + len(inode.value)

		if inode.flags&bucketLeafFlag != 0 {
			return false
		} else if size > b.maxInlineBucketSize() {
			return false
		}
	}

	return true
}
// Returns the maximum total size of a bucket to make it a candidate for inlining.
func (b *Bucket) maxInlineBucketSize() int {
	return b.tx.db.pageSize / 4
}
```

`inline bucket` 在父 `bucket` 的存放的格式如下，可以看出来是把完整的 `page` 格式追加到了 `bucket header` 后面，很好的复用了代码：
![image](/assets/images/boltdb/inlineBucket.png)

### B+ 树的平衡
`boltdb` 事务中的所有操作都是在内存中进行，比如删除或增加 `key/val` 都是操作对应 `leaf node` 的 `inodes`。直到事务提交时，才会进行 `B+` 树的分裂与合并，这样可以避免事务回滚时做无用功。

`B+` 树的平衡分为2步：
1. 合并结点(`rebalance`)：会将任何删除过 `key` 且大小和 `key` 数量不满足要求的 `node` 与 `sibiling` 合并。
2. 分裂结点(`spill`)：将大小超过 `page size * FillPercent` 的 `node` 分解为多个 `node`。

这里也不再详细介绍实现，重点是理解 `subbucket` 和 `parent bucket` 的关系还有递归。`subbucket` 对于 `parent bucket` 就是 `leaf node` 的一对 `K/V`，
在合并的时候，会先把当前 `Bucket` 在事务中遍历到的所有 `node` 进行合并，然后再递归地合并 `subbucket`；在分裂的时候，会首先分裂所有的 `subbucket` 然后才是当前 `Bucket` 中的 `node`。

### metadata
`metadata` 存放在文件的前2个 `page`，存储整个 `db` 的信息，比如 `B+` 树的 `root` 结点位置、`freelist` 的位置、`page size` 等，结构如下:
```go
type meta struct {
	magic    uint32
	version  uint32
	pageSize uint32
	flags    uint32
	root     bucket // B+ 树 root 结点的 page id
	freelist pgid // 存放 freelist 的 page id
	pgid     pgid // db 目前使用的 page 个数
	txid     txid // 最新提交的事务 id，相当于版本号
	checksum uint64
}
```

## 事务
`boltdb` 支持完整的事务特性(`ACID`)，使用 `MVCC` 并发控制，允许多个读事务和一个写事务并发执行，但是读事务有可能会阻塞写事务。它的实现方式如下：
* `Durability`: 写事务提交时，会为该事务修改的数据(`dirty page`)分配新的 `page`，写入文件。
* `Atomicity`: 未提交的写事务操作都在内存中进行；提交的写事务会按照 `B+` 树数据、`freelist`、`metadata` 的顺序写入文件，只有 `metadata` 写入成功，整个事务才算完成，只写入前两个数据对数据库无影响。
* `Isolation`: 每个读事务开始时会获取一个版本号，读事务涉及到的 `page` 不会被写事务覆盖；提交的写事务会更新数据库的版本号。

`tx` 的结构如下：
```go
// Tx represents a read-only or read/write transaction on the database.
// Read-only transactions can be used for retrieving values for keys and creating cursors.
// Read/write transactions can create and remove buckets and create and remove keys.
//
// IMPORTANT: You must commit or rollback transactions when you are done with
// them. Pages can not be reclaimed by the writer until no more transactions
// are using them. A long running read transaction can cause the database to
// quickly grow.
type Tx struct {
	writable bool
	managed  bool
	db       *DB
	meta     *meta
	root     Bucket
	pages          map[pgid]*page
	stats          TxStats
	commitHandlers []func()

	// WriteFlag specifies the flag for write-related methods like WriteTo().
	// Tx opens the database file with the specified flag to copy the data.
	//
	// By default, the flag is unset, which works well for mostly in-memory
	// workloads. For databases that are much larger than available RAM,
	// set the flag to syscall.O_DIRECT to avoid trashing the page cache.
	WriteFlag int
}
```

### Commit
一个写事务提交的流程如下:
1. 平衡 `B+` 树，在分裂过程中会为每个修改过的 `node` 分配新的 `page`；
2. 为 `freelist` 分配新的 `page`；
3. 将 `B+` 树数据和 `freelist` 数据写入文件；
4. 写入 `metadata`。

### MVCC

## 总结
