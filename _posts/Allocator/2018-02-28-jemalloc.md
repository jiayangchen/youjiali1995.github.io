---
title: jemalloc 源码结构
layout: post
categories: Allocator
---

{% include toc %}

## Redis 编译参数
```
./configure --with-lg-quantum=3 --with-jemalloc-prefix=je_ --enable-cc-silence CFLAGS="-std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops " LDFLAGS=""
ar crus lib/libjemalloc.a src/jemalloc.o src/arena.o src/atomic.o src/base.o src/bitmap.o src/chunk.o src/chunk_dss.o src/chunk_mmap.o src/ckh.o src/ctl.o src/extent.o src/hash.o src/huge.o src/mb.o src/mutex.o src/pages.o src/prof.o src/quarantine.o src/rtree.o src/stats.o src/tcache.o src/util.o src/tsd.o src/valgrind.o src/zone.o

1. --with-lg-quantum=<lg-quantum>: Base 2 log of minimum allocation alignment. 8字节对齐
2. --with-jemalloc-prefix=<prefix>: Prefix to prepend to all public APIs. 每个 pulic apis 增加前缀？
3. --disable-cc-silence: Do not silence irrelevant compiler warnings. 开启编译警告
```

## 自己看代码的时候
```
./configure --with-lg-quantum=3 --enable-debug --enable-cc-silence CFLAGS="-std=gnu99 -Wall -pipe -g3 -O0" LDFLAGS=""
```

## 备忘
向上舍入: `(n + mask) >> lg_size, mask = (1 >> lg_size) - 1`

## 锁
mutex.h: mutex 使用 spinlock:
```c
JEMALLOC_INLINE void
malloc_mutex_lock(malloc_mutex_t *mutex)
{

	if (isthreaded) {
#ifdef _WIN32
#  if _WIN32_WINNT >= 0x0600
		AcquireSRWLockExclusive(&mutex->lock);
#  else
		EnterCriticalSection(&mutex->lock);
#  endif
#elif (defined(JEMALLOC_OSSPIN))
		OSSpinLockLock(&mutex->lock);
#else
		pthread_mutex_lock(&mutex->lock);
#endif
	}
}
```

## 数据结构
1. rb: 红黑树
2. rtree: radix tree
3. qr: ring
4. ql: list
5. tsd: 根据配置使用 tls (线程局部存储\_\_thread)或 tsd (线程特有数据 pthread_key_create(), pthread_setspecific())

## 配置宏
```c
/* sizeof(void *) == 2^LG_SIZEOF_PTR. */
#define LG_SIZEOF_PTR 3

/* Minimum size class to support is 2^LG_TINY_MIN bytes. */
#define LG_TINY_MIN 3

/*
 * Minimum allocation alignment is 2^LG_QUANTUM bytes (ignoring tiny size
 * classes).
 */
#define LG_QUANTUM 3

/* One page is 2^LG_PAGE bytes. */
#define LG_PAGE 12
```

## 种类
内存按 chunk 分配，大于 page size

1. User objects are broken into three categories according to size: small, large, and huge. Small and large objects are managed entirely by arenas;
2. huge objects are additionally aggregated in a single data structure that is shared by all threads.
   Huge objects are typically used by applications infrequently enough that this single data structure is not a scalability issue.
3. Small objects are managed in groups by page runs. Each run maintains a bitmap to track which regions are in use. Allocation requests
that are no more than half the quantum (8 or 16, depending on architecture) are rounded up to the nearest power of two that is at least sizeof(double).
All other object size classes are multiples of the quantum, spaced such that there are four size classes for each doubling in size, which limits internal fragmentation to
approximately 20% for all but the smallest size classes. Small size classes are smaller than four times the page size,
large size classes are smaller than the chunk size (see the "opt.lg_chunk" option), and huge size classes extend from
the chunk size up to one size class less than the full address space size.


## 初始化
第一次分配时进行初始化
je_malloc() -> imalloc_body() -> malloc_init() -> malloc_init_hard()
从3个地方获取配置:
1. 文件 /etc/malloc.conf
2. 环境变量 MALLOC\_OPTIONS
3. 全局变量 \_malloc_options

初始化
1. malloc_init_hard_a0_locked():
    1. base: 初始化 extent_tree_t 红黑树， node 为 extent_node_s
    2. chunk: 设置 chunk 大小，初始化 chunks_rtree(radix tree)，chunks_tree 保存 chunk 地址到 extent_node_t 的映射？
    3. ctl: 初始化 mutex
    4. arena: bin_info_init() 设置 bin 信息 `arena_bin_info_t`, small_run_size_init()
    5. tcache: 设置 tcache 中每个 bin 能分配的 slot 最大值
    6. arena_init(): 分配 arena0
2. malloc_tsd_boot0():
3. malloc_init_hard_finish(): 设置 narenas，初始化 arena 相关的信息

## 内存分配
(有待商榷)内存大小都会向上近似 ceiling 为 L1 cacheline (64 bytes) 的整数倍，避免 cache line aliasing\false cache line sharing。

```c
// 就是 size classes 还是 bins?
size_t index2size_tab[] = {
    8, 16, 24, 32,
    40, 48, 56, 64,
    80, 96, 112, 128,
    160, 192, 224, 256,
    320, 384, 448, 512,
    640, 768, 896, 1024,
    1280, 1536, 1792, 2048,
    2560, 3072, 3584, 4096, // LOOKUP_MAXCLASS
    ...
};

uint8_t size2index_tab[] = {
    0, 1, 2, 3,
    4, 5, 6, 7,
    8, 8, 9, 9, 10, 10, 11, 11,
    12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15,
    ...
    31
};
```
  
## extent
`extent_tree_t` 红黑树
`extent_node_t` 管理一块 `chunk`

## chunk
`chunksize = 1 << opt_lg_chunk(21)`

`chunks_rtree` 是用来管理 `chunk` 的吗？

## page
`pagesize = 1 << LG_PAGE(12)`


`JEMALLOC_THREADED_INIT undefined`

extent_tree_t 是 rb_tree

## 内部分配
`base_alloc()`?

## tsd
`tsd` 用于保存每个线程特有的数据(主要是 `arena` 和 `tcache`)，避免锁的竞争。`tsd_t` 中的数据会在第一次访问时延迟初始化(调用相应的 `get_hard()`)，`tsd` 中各元素使用宏生成对应的 `get/set` 函数来获取/设置，在线程退出时，会调用相应的 `cleanup` 函数清理。下面只介绍 `linux` 平台中的实现。  
在 `linux` 中会使用 `tls(__thread)` 和 `tsd(pthread_key_create(), pthread_setspecific())` 来实现:
```c
#elif (defined(JEMALLOC_TLS))
#define	malloc_tsd_data(a_attr, a_name, a_type, a_initializer)		\
a_attr __thread a_type JEMALLOC_TLS_MODEL				\
    a_name##tsd_tls = a_initializer;					\
a_attr pthread_key_t	a_name##tsd_tsd;				\
a_attr bool		a_name##tsd_booted = false;
```

* `__thread` 保存需要线程局部存储的数据
* `pthread_key_t` 用于注册 `destructor`，在线程退出时清理 `tsd`  
其实可以只用 `pthread_key_t` 来实现，但使用 `__thread` 可以直接获取数据，不用再调用 `pthread_getspecific()`。  

### 线程退出
线程退出时，会调用 `tsd_cleanup()` 对 `tsd` 中数据进行清理:
* `arena`:  降低 `arena` 负载(`arena->nthreads--`)
* `tcache`: 调用 `tcache_bin_flush_small/large()` 释放 `tcache->tbins[]` 所有元素，释放 `tcache` 本身

## tcache
`tcache` 划分为多个 `bin`，每个 `bin` 划分为多个 `cache slot`。
`tcache_boot()`: 初始化 `tcache_bin_info_t` 记录了每个 `bin` 的最大 `cache slot` 个数，和总共的 `cache slot` 个数
`tcache_create()`: 根据 `tcache_boot()` 的数据分配 `tcahce_t`，分配了一块连续的内存，`tcache_bin_t` 中的 `avail` 先指针对齐，然后指向一段内存


## 内存分配大小
`CACHELINE_CEILING` 向上舍入 `cacheline` 大小，防止 false cache line sharing
`s2u()` 查找 size_classes 确定分配大小，会向上舍入:
  * 小于 `LOOKUP_MAXCLASS(4096)` 的查表
  * 大于的计算
一般会先 `CACHELINE_CEILING` 再 `s2u()`，到指定 `size_classes` 的大小？

## 分配
`arena_malloc()`

## bin
memory 按 chunk 申请，大于 page，(arena 将 chunk 划分为 run)? arena 管理 bin，bin 管理 run，run 再分配为 region, run 大小是 page 倍数？
每个 region 的大小为 bin 对应的size，从 size_classes 里拿


## pages.c
最底层的分配，封装了不同平台的系统调用，linux 使用 `mmap()`。提供了几个函数:
  1. `pages_map()`: 调用 `mmap()` 分配可读可写、私有匿名映射。
  2. `pages_unmap()`: 调用 `mummap()` 删除指定范围的映射。
  3. `pages_trim()`: `trim` 头尾部分的内存映射，用于内存对齐。
  4. `pages_purge()`: 调用 `madvise()` 清除(purge)部分内存页，也就是释放。  

`mmap()` 会按照 `pagesize` 的倍数分配内存，匿名映射会初始化为0，私有映射采用 `COW` 策略。

## base.c
`rbtree` `nsearch()` 返回相等或最接近的比它大的。
内部的内存分配？

`base` 使用红黑树作为查找合适内存的数据结构，树的每个 `node` 负责一块内存，记录内存的大小和起始地址。每次需要分配时，会从红黑树中查找内存大小相同或略大的、地址最低的 `node`，
然后从 `node` 负责的 `extent` 中分配内存，剩下的内存会继续由该 `node` 负责，修改大小和地址后再次插入到红黑树中；若该 `node` 负责的内存全部分配完了，会将该 `node` 添加到链表头 `base_nodes`，留待
后续分配时复用。当没有合适的 `node` 时，会新分配 `chunk` 大小倍数的内存，由 `node` 负责，这个 `node` 优先从链表中分配，也可能是新分配的连续内存的起始位置构成的 `node`。

`base_alloc()`: 从 `extent_tree_szadd_t` 中查找大小相同或略大的、地址最低的 `extent_node_t`，再从 `node` 里分配内存。
如果没有合适的内存，会先调用 `base_chunk_alloc()` 分配 `chunk` 大小倍数的内存，返回负责这块内存的 `node`，然后进行分配。
```c
	ret = extent_node_addr_get(node); /* node 中用于分配内存的起始地址 */
	if (extent_node_size_get(node) > csize) {
		extent_node_addr_set(node, (void *)((uintptr_t)ret + csize)); /* 起始地址增加 csize，表明之前的内存被分配出去 */
		extent_node_size_set(node, extent_node_size_get(node) - csize); /* 内存大小减少 */
		extent_tree_szad_insert(&base_avail_szad, node); /* 按照大小、地址顺序插入到红黑树 */
	} else
        /* 这种情况应该只发生在 extent_node_size_get(node) == csize 这种情况。
         * 此时该 node 负责的内存已经全部分配了，会将该 node 插入到一个链表中去，备用。
         * 该链表用嵌入式实现，在 node 的起始内存存放下一个 node 的地址，节省空间 */
		base_node_dalloc(node);
```

`base_chunk_alloc()` -> `chunk_alloc_base()` -> `chunk_alloc_mmap()` -> `pages_map()`。 `chunk_alloc_map()` 会分配地址按 `chunk_size` 对齐的内存，但是
`pages_map()` 不能保证对齐，首先会调用 `pages_map()` 分配一块内存查看是否对齐，若没对齐，会重新多分配一些内存，然后调用 `pages_trim()` 截取两端使内存对齐
，所以可能会有多次 `mmap()` 和 `munmap()` 的过程。
`base_chunk_alloc()` 会分配 `chunk_size` 倍数大小的内存，有可能会多分配出 `extent_node_t` 的空间，`extent_node_t *` 指向内存起始地址，剩下的内存由该 `node` 负责进行分配。

`base_nodes` 形成了一个 `node` 的链表，`base_nodes` 指向链表头，`base_node_dalloc()` 将 `node` 添加到表头，而 `base_node_try_alloc` 移除表头。这是为了复用之前多余的
`extent_node_t`，减少内存空间的浪费。

## arena

![image](/assets/images/arena.png)

`arena_boot()`: `arena` 根据 `size_classes` 来分配内存，`arena_boot()` 首先计算 `nlclasses(large classes 的个数)` 和 `nhclasses(huge classes 的个数)`。
内存分配都是整块连续的 `chunk` 大小倍数的内存，`small classes` 和 `large classes` 都从 `chunk` 中进行分配，`chunk` 又被划分为 `page` 进行管理。`arena_chunk_t` 中的 `map_bits[]` 记录这块 `chunk` 中每个 `page` 的信息，
`arena_chunk_map_misc_t` 也一一对应 `chunk` 中的 `page` ，这些记录 `page` 信息的 `header` 和 `chunk` 一同分配，所以会占用掉部分内存，而这些内存又和 `chunk` 中的 `page` 个数有关，而 `chunk` 中减去 `header` 的内存又和 `page` 的个数有关，
所以 `arena_boot()` 中使用循环计算 `header` 占用的 `page` 个数(`map_bias`):
```c
	/*
	 * Compute the header size such that it is large enough to contain the
	 * page map.  The page map is biased to omit entries for the header
	 * itself, so some iteration is necessary to compute the map bias.
	 *
	 * 1) Compute safe header_size and map_bias values that include enough
	 *    space for an unbiased page map.
	 * 2) Refine map_bias based on (1) to omit the header pages in the page
	 *    map.  The resulting map_bias may be one too small.
	 * 3) Refine map_bias based on (2).  The result will be >= the result
	 *    from (2), and will always be correct.
	 */
	map_bias = 0;
	for (i = 0; i < 3; i++) {
		size_t header_size = offsetof(arena_chunk_t, map_bits) +
		    ((sizeof(arena_chunk_map_bits_t) +
		    sizeof(arena_chunk_map_misc_t)) * (chunk_npages-map_bias));
        // 向上舍入
		map_bias = (header_size + PAGE_MASK) >> LG_PAGE;
	}
```

### arena_chunk_t(重要)
一整块 `chunk(arena_chunk_t)` 的内存分为3个部分(典型大小为 `1 << 21(2MB)`):
1. 最开始的 `arena_chunk_t` 的 `node`
2. 每 `page` 一个的 `map_bits`(数组)
3. 每 `page` 一个的 `arena_chunk_map_misc_t`(数组)，对应着每个 `run`。`arena_run_t` 只记录该 `run` 的信息，`run` 的内存为 `chunk` 中对应的 `page` 的起始内存
4. 每个 `page` 对应着一个 `run`，`run` 的起始 `page` 对应的 `run` 为真正使用的 `run`。每种 `bin` 对应的 `run` 的信息保存在全局数组 `arena_bin_info[NBINS]` 中，内存格式就对应着注释的图。分配时就是分配一个 `region`。
`small size classes` 用 `bin` 进行分配，`bin` 管理着 `run`，`run` 对应着 `chunk` 内多个 `page`，`arena_run_t` 是对应 `run` 起始 `page` 的那个，`arena_chunk_map_bits_t` 记录了 `run` 中每个
`page` 的信息。

### arena_chunk_map_bits_t
在64位系统上，共有 `64bits`，记录了 `chunk` 内每个 `page` 的分配情况，对于不同状态的 `page` 有不同的格式:
1. 未分配的 `run` 对应的 `page`: 连续、未分配的 `page` 会作为一个整体，由起始 `page` 对应的 `run` 进行管理。首尾 `page` 对应的 `arena_chunk_map_bits_t` 中会设置连续的空闲 `page` 的数量，
中间的 `page` 不设置。同时，管理这些空闲 `page` 的 `run` 会插入到 `runs_avail` 中，该 `run` 的大小就是整个空闲 `page` 的大小(从 `arena_chunk_map_bits_t` 中获取)
2. 已分配的 `run` 对应的 `page`: 每个 `page` 会设置该 `page` 是 `run` 中第几个 `page(run page offset)`，并且设置 `run` 对应的 `binInd`

### 从 `chunk` 中分配 `run`
获取的 `chunk` 都是空闲的，`chunk` 中所有的 `page` 都由第一个 `page` 对应的 `run` 管理，首尾 `page` 会设置整个 `chunk` 中 `page` 的数量，分配 `chunk` 其实就是创建了一个 `run_size` 最大的 `run` 并插入到 `avail_runs` 中，
之后会分解该 `run`(这就是伙伴算法？)  
然后会将该 `run` 分解，调用 `arena_run_split_small()` 分解为对应的 `bin_info` 中 `run_size` 的已分配的 `run` 并返回，剩下的会进行设置(首尾 `page` 设置大小)，然后插入到 `runs_avail` 中，留待后续分配。
`avail_runs` 中的结点都是连续空闲的起始 `page` 对应的 `run`，该 `run` 对应的大小会从对应的 `arena_chunk_map_misc_t` 中获取。

### small size classes 分配流程
1. 查找对应 `size classes` 的 `bin`
2. 从 `bin->runcur` 中分配 `region`
3. 从 `bin->runs` 查找未满的 `run`
4. 从 `arena->avail_runs` 中查找空闲 `run`
5. 从 `chunk` 中分配 `run`，首先查找空闲的 `chunk`:
    1. `arena->spare`
    2. `arena->cached_tree`
    3. `arena->retained_tree`
    4. 调用 `mmap()` 新分配一块 `chunk`
    5. 2-3 中分配的 `chunk` 都会调用 `arena_chunk_register()` 插入到 `chunks_rtree` 中，作甚？


`arena` 中 `extent_tree_t` 是 `arena_chunk_t` 的树，因为 `arena_chunk_t` 的第一个字段就是 `extent_node_t`。

`CHUNK_ADDR2BASE()`: 获取整块 `chunk` 的起始地址，因为 `chunk` 按照 `chunk` 大小分配，且按照 `chunk` 大小对齐，这是获取 `chunk` 大小倍数的最近的地址
`map_misc_offset`: `arena_chunk_map_misc_t` 到 `chunk` 起始地址的偏移

`bin_info_init()` 根据 `size_classes` 初始化 `small class bins` 的信息 `arena_bin_info[NBINS]`。数组中每个元素记录了 `run` 对应的 `bin` 的信息，包含对应的 `region` 大小、`region` 个数、`run` 大小等:
* `reg_size`: 每个 `region` 的大小，对应着 `small size classes` 大小
* `run_size`: `bin` 对应的整个 `run` 的大小，`page_size` 的倍数，一般为 `reg_size` 和 `page_size` 的最小公倍数，但是不能超过 `arena_maxrun`。
* `nregs`: 该 `run` 中 `region` 的个数

`small_run_size_init()` 初始化 `small_run_tab[]`，`run` 大小 `page` 倍数对应索引的为 `true`。这他妈是啥？

在 `arena` 一开始初始化时，只会分配1个 `arena: a0`，其余的 `lazy init`。 `arena_new()` 为创建 `arena` 的函数，初始化结构体，主要是创建了很多红黑树，`arena` 中管理的内存会按需分配。
在 `malloc_init_hard_finish()` 中会设置 `arena` 的相关配置，`narenas_auto` 和 `narenas_total` 都设置为 `cpu` 核数*4，默认最多创建那么多 `arena`。

`je_malloc()` 最终会调用 `arena_malloc()` 分配内存：
```c
JEMALLOC_ALWAYS_INLINE void *
arena_malloc(tsd_t *tsd, arena_t *arena, size_t size, bool zero,
    tcache_t *tcache)
{

	assert(size != 0);

	arena = arena_choose(tsd, arena);
	if (unlikely(arena == NULL))
		return (NULL);

	if (likely(size <= SMALL_MAXCLASS)) {
		if (likely(tcache != NULL)) {
            // 从线程缓存中分配
			return (tcache_alloc_small(tsd, arena, tcache, size,
			    zero));
		} else
            // arena 分配
			return (arena_malloc_small(arena, size, zero));
	} else if (likely(size <= large_maxclass)) {
		/*
		 * Initialize tcache after checking size in order to avoid
		 * infinite recursion during tcache initialization.
		 */
		if (likely(tcache != NULL) && size <= tcache_maxclass) {
			return (tcache_alloc_large(tsd, arena, tcache, size,
			    zero));
		} else
			return (arena_malloc_large(arena, size, zero));
	} else
		return (huge_malloc(tsd, arena, size, zero, tcache));
}
```
会首先调用 `arena_choose()` 选择一个 `arena` 来分配。选择 `arena` 的逻辑如下:
1. 若有空闲的(`nthreads==0`)已创建的 `arena`，则选择该 `arena`
2. 若还有未创建的 `arena`，则选择新创建一个 `arena`
3. 选择负载最低的 `arena` (`nthreads` 最小)  
之后会把该 `arena` 绑定到 `tsd` 中，就由该 `arena` 负责这个线程的分配，之后 `arena_choose()` 就会直接从 `tsd` 中获取 `arena`，避免多线程的锁竞争。

根据分配内存的大小，选择不同的分配方式:
1. 小于 `SMALL_MAXCLASS`
2. 在 `SMALL_MAXCLASS` 和 `large_maxclass` 之间
3. 大于 `large_maxclass`
前2类分配方式为：若支持 `tcache` 会优先从 `tcache` 中分配，否则从 `arena` 中分配；第3类使用所有线程公用的数据结构来分配，因为 `huge` 分配的频率很低，
公用一个数据结构不会成为瓶颈。

`arena` 从对应的 `small classes` 的 `bin` 中的 `run` 分配 `region`。首先会从 `bin->runcur` 分配，若没有，则会重新获取 `run`，再从该 `run` 中分配。获取 `run` 会经历下面阶段:
1. `arena_bin_nonfull_run_tryget()`: 从 `bin` 中分配 `run`，返回红黑树 `bin->runs` 中第一个节点
2. `arena_run_alloc_small()`: 从 `arena` 中分配 `run`:
  * `arena_run_alloc_small_helper()`: 返回红黑树 `arena->runs_avail` 中合适节点，还会调用 `arena_run_split_small()` 将多余的空间设为新的 `run` 插入
  * 先创建一个 `chunk`，`chunk` 相当于一个大小为 `arena_maxrun` 的 `run`，然后对该 `run` 进行分解，剩下的插入到 `arena->avail_runs` 中备用:
    * `chunk` 首先是 `arena->spare`
    * 其次调用 `arena_chunk_init_hard()` 分配 `chunk`:
      * `chunk_alloc_cache()`: 调用 `chunk_recycle()` 从 `arena` 中 `cached tree` 中查找
      * `arena_chunk_alloc_internal_hard()`: 调用 `chunk_recycle()` 从 `arena` 中 `retained tree` 中查找，没有合适的会调用 `chunk_alloc_mmap()` 新分配一个 `chunk`

`arena_malloc_small()`: 先查找要分配 `size` 对应的 `bin`，然后从该 `bin` 的 `run` 中进行分配。 `arena_bin_t` 中维护了一个红黑树用于管理 `runs`，红黑树中
保存着未满的 `run`。分配会首先使用上次分配时使用的 `runcur`，若没有，则会重新选择一个，先从 `runs` 中选择，没有合适的会调用 `arena_run_alloc_small()` 新分配一个。

`arena_run_alloc_small()`:  首先调用 `arena_run_alloc_small_helper()` 从 `avail_runs` 中查找合适的 `run`；否则，调用 `arena_chunk_alloc()` 分配一个 `chunk`，从中分配一个 `run`。

`arena_chunk_alloc()`: 使用缓存的 `spare` 或者调用 `arena_chunk_init_hard()` 新分配一个 `chunk`，然后插入到 `avail_trees` 中

`arena_chunk_init_hard()`: 申请 `chunk` 内存，初始化 `arena_chunk_t` 中 `extent_node_t`，设置 `map_bits`，并将 `arena_chunk` 插入到 `chunks_rtree` 中，`rtree` 保存 `chunk` 起始地址到 `extent_node_t` 的映射

`chunk_alloc_xxx()`: 从什么地方分配 `chunk`，`chunk_alloc_cache()` 从 `arena_t` 的 `cached tree` 中分配，`chunk_alloc_core()` 从 `arena_t` 的 `retained tree` 中分配，`chunk_alloc_base()` 从 `base` 分配。

`chunk_recycle()`:  `chunk_alloc_cache()` 和 `chunk_alloc_core()` 会调用这个函数，从对应的 `arena_t` 中的2个树中进行分配指定大小的 `chunk`，`chunk` 起始地址会按 `chunk_size` 对齐。因为需要对齐，前后需要进行裁剪，
`leadsize` 和 `trailsize` 也会重新插入树中，留待之后的分配使用。除了插入树中，还会插入到 `runs_dirty` 和 `chunks_cache` 中，分别对应 `extent_node_t` 的 `rd` 和 `cc_link`。干啥用的？是个环

`arena_chunk_register()`: 

`arena_run_split_small()`: `chunk` 中划分一个 `run` 返回，再讲剩余的 `run` 插入到树中 `avail_runs`

`arena_run_reg_alloc()`: 从 `run` 中分配一个 `region`:
1. 根据 `arena_run_t` 中的 `bitmap` 查找未分配的 `region`。 `arena_run_t` 只记录了 `run` 的信息，没有内存地址
2. 获取该 `run` 的起始地址(内存格式为：分配的整块的 `chunk`，起始地址为 `arena_chunk_t` 结构体，然后是每 `page` 一个的 `arena_chunk_map_bits_t`，每 `page` 一个的 `arena_chunk_map_misc_t`，
`run` 的起始地址就是对应的 `page` 的地址，`run` 就是 `page`?)
3. 获取起始地址后，根据 `bin_info` 返回对应 `region`

分配策略:
1. 根据大小选择 `binid`
2. 从对应 `run` 中分配 `region`

## free
`je_free()` 最终调用 `arena_dalloc()` 来释放内存，释放的策略一一对应分配时的策略：
```c
JEMALLOC_ALWAYS_INLINE void
arena_dalloc(tsd_t *tsd, void *ptr, tcache_t *tcache)
{
	arena_chunk_t *chunk;
	size_t pageind, mapbits;

	assert(ptr != NULL);

	chunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);
	if (likely(chunk != ptr)) {
		pageind = ((uintptr_t)ptr - (uintptr_t)chunk) >> LG_PAGE;
		mapbits = arena_mapbits_get(chunk, pageind);
		assert(arena_mapbits_allocated_get(chunk, pageind) != 0);
		if (likely((mapbits & CHUNK_MAP_LARGE) == 0)) {
			/* Small allocation. */
			if (likely(tcache != NULL)) {
				szind_t binind = arena_ptr_small_binind_get(ptr,
				    mapbits);
				tcache_dalloc_small(tsd, tcache, ptr, binind);
			} else {
				arena_dalloc_small(extent_node_arena_get(
				    &chunk->node), chunk, ptr, pageind);
			}
		} else {
			size_t size = arena_mapbits_large_size_get(chunk,
			    pageind);

			assert(config_cache_oblivious || ((uintptr_t)ptr &
			    PAGE_MASK) == 0);

			if (likely(tcache != NULL) && size - large_pad <=
			    tcache_maxclass) {
				tcache_dalloc_large(tsd, tcache, ptr, size -
				    large_pad);
			} else {
				arena_dalloc_large(extent_node_arena_get(
				    &chunk->node), chunk, ptr);
			}
		}
	} else
		huge_dalloc(tsd, ptr, tcache);
}
```

`arena_dalloc_small()`: `small size classes` 的释放很简单:
1. 调用 `arena_run_reg_dalloc()`: 获取该 `ptr` 对应的 `regind`，然后清除 `run` 中 `bitmap`，增加 `nfree`
2. 如果 `run->nfree == bin_info->nregs`，表明该 `run` 中所有 `region` 都未分配，会调用 `arena_dissociate_bin_run()` 将该 `run` 与对应的 `bin` 分离:
    1. 如果 `run` 为 `bin->runcur`，`bin->runcur` 设为 `NULL`
    2. 否则，将 `run` 从 `bin->runs` 中移除
    3. 2若为真，则调用 `arena_run_dalloc()` 释放该 `run` 和对应的 `page`(释放就是设置对应的 `arena_chunk_map_misc_t`)，然后调用 `arena_run_coalesce()` 尝试与 `chunk` 中前后相邻的空闲 `run` 进行合并(伙伴算法)(知道为啥要在空闲 `run` 的首尾 `page` 设置 `page` 大小了，为了快速的找到 `run` 的起始
    `page`)，然后插入到 `arena->avail_runs`。若发现合并完之后，整个 `chunk` 都为空，调用 `arena_chunk_dalloc()` 释放 `chunk`，调用 `chunk_dalloc_cache()->chunk_record()` 将 `chunk` 插入到 `arena->cached_tree` 中(`chunk` 其实就是 `extent`)，同样会进行合并，
  连续地址空间的都在 `tree` 中的会合并，由 `chunk_record()` 合并
3. 如果 `run->nfree == 1`，调用 `arena_bin_lower_run()` 将该 `run` 插入到 `bin->runs` 中或者设置为 `bin->runcur` 并将原先的 `runcur` 插入到 `bin->runs`中(`bin->runs` 中保存的是未满的 `run`，`bin->runcur` 指向的是地址最低的 `run`)

`chunk_record()`: 将 `chunk` 插入到 `extent_tree_t` `chunks_szad` 和 `chunk_ad` 中，`chunk` 就是 `arena_chunk_t`，该结构体的首个字段就是 `extent_node_t`，所以可以用 `extent_tree_t` 代表。
会查找连续地址空间的前后的 `chunk` 在不在树中，如果在的话会进行合并，然后再插入到树中。

在 `arena_run_regind()` 中有下面一段注释，影响竟然那么大:
```
	/*
	 * Avoid doing division with a variable divisor if possible.  Using
	 * actual division here can reduce allocator throughput by over 20%!
	 */
```

`cached` 才是 `dirty` ? `retained` 不是?
`arena_run_dirty_insert()` 这他喵的干啥用的？ `dirty` 的 `run` 插进去？
`chunk` 也有 `arena_chunk_cache_maybe_insert()` `arena_chunk_cache_maybe_remove()`，只有 `cached tree` 中才调用

## purge
只有在释放的时候才会进行 `purge`，因为只有这时候 `ndirty` 才会增加。释放会调用 `pages_purge()` 释放内存。
`arena` 中 `lg_dirty_mult`、`nactive`、`ndirty` 决定何时 `purge`  
`arena_maybe_purge()`:

## arena large
分配 `large` 和分配 `small` 类似，不过 `large` 的大小比 `chunk` 小，比 `region` 都大，所以单个 `run` 只分配一个 `large object`，分配的过程也类似:
1. 先从 `arena->avail_runs` 中查找，因为 `large object` 不由 `bin` 管理，所以与 `small object` 相比，少了从 `bin->runs` 中查找的一步
2. 分配 `chunk`，步骤和 `small object` 一样，然后从 `chunk` 中分配需要的 `run` 大小，此时 `run` 的大小为单个 `object` 的大小，而 `small run` 的大小会从 `bin_info[]` 中获取
3. `large` 和 `small` 的 `arena_chunk_map_misc_t` 格式也不同。`large` 只在首个 `page` 设置 `run` 的大小，末尾 `page` 设置 `0`。

`large` 比 `chunk` 小，一个 `run` 只分配一个 `large object`，策略和 `small` 大同小异

## free large
和 `small` 一样，只是缺少了 `run` 在 `bin` 中的处理。直接调用 `arena_run_dalloc()`

## huge
`huge object` 大小比 `chunk` 大。分配策略和上面分配 `chunk` 一样，按照 `chunk` 进行分配，从 `retained` 或调用 `chunk_alloc_mmap()` 分配，此时的 `chunk` 内存全部用于分配，管理 `chunk` 的头部 `extent_node_t` 会另外分配，`chunk` 同样会调用 `chunk_register()` 插入到
`chunks_rtree` 中，并且 `huge object` 会在 `arena->huge` 链表中管理。

## free huge
释放和分配过程相反:
1. 将 `node` 从 `chunks_rtree` 中移除
2. 移出 `arena->huge`
3. 释放 `chunk`，插入到 `arena->cached_tree` 中
4. 释放 `node`

## chunk
基数树 `chunks_rtree` 是为了获取 `ptr(chunk)` 到 `node(extent_node_t)` 的映射，然后可以找到 `chunk` 的信息，比如 `arena`、大小等。

## tcache
`tcache` 可用于 `small` 和 `large` 的分配，避免多线程的同步。
```
   "opt.tcache" (bool) r- [--enable-tcache]
           Thread-specific caching (tcache) enabled/disabled. When there are multiple threads, each thread uses a tcache for objects up to a certain size. Thread-specific caching allows many allocations to be satisfied without performing any
           thread synchronization, at the cost of increased memory use. See the "opt.lg_tcache_max" option for related tuning information. This option is enabled by default unless running inside Valgrind[2], in which case it is forcefully
           disabled.

    "opt.lg_tcache_max" (size_t) r- [--enable-tcache]
        Maximum size class (log base 2) to cache in the thread-specific cache (tcache). At a minimum, all small size classes are cached, and at a maximum all large size classes are cached. The default maximum is 32 KiB (2^15).
```

`tcache_boot()`: 根据配置 `opt.lg_tcache_max` 设置 `tcache` 中 `bin` 的范围(`nhbins`)。 设置 `tcache_bin_info`，保存每种 `bin` 的 `cache slots` 个数(类似 `arena_bin_info` 中 `nregs`)，`small` 在 `TCACHE_NSLOTS_SMALL_MIN` 到 `TCACHE_NSLOTS_SMALL_MAX` 间，
`large` 固定为 `TCACHE_NSLOTS_LARGE`。

`tcache` 从 `tsd` 中获取，在第一次获取(null)时创建 `tcache_create()`。

`tcache_create()`: `tcache_t` 中保存着 `tbins[]` 信息，`tcache_bin_t` 中 `avail` 指向每一个 `cache slot`(类似 `arena->bin` 中 `region`)，`tcache_create()` 根据 `tcache_boot()` 设置的配置分配 `tcache_t` 和 `tcache_bin_t` 的内存，
`tcache_t` 和 `tbins[]` 为连续内存，`tbins[]` 中 `avail` 使用后面连续空间的内存。

![image](/assets/images/tcache.png)

### tcache small
#### 分配
`tcache_alloc_small()`: 先获取对应的 `tbin`，调用 `tcache_alloc_easy()`，若 `tbin` 中还有剩余的元素，返回 `tbin->avail[tbin->ncached]`(从后往前分配，`ncached` 既是剩余数量也是索引)，`tbin->low_water` 保存着 `tbin->ncached` 的最小值。  
`tcache_alloc_small_hard()`: `tbin` 已空，先调用 `arena_tcache_fill_small()` 重新装载 `tbin`，在调用 `tcache_alloc_easy()` 分配。  
`arena_tcache_fill_small()`: 从 `arena` 中对应的 `bin` 分配 `region` 保存在 `tbin->avail` 中，只会填充 `ncached_max >> lg_fill_div` 个。

#### 释放
`tcache_dalloc_small()`: 通过 `ptr` 对应的 `map_bits` 获取 `binind`，然后将 `ptr` 释放(保存在 `tbin->avail[tbin->ncached]`，同时 `tbin->ncached++`)。若该 `tbin` 已满(`tbin->ncached == tbin_info->ncached_max`)，会调用 `tcache_bin_flush_small()`。  
`tcache_bin_flush_small()`: 会释放 `tbin` 中一半的 `avail` 返回给 `arena` 中对应的 `bin`，这里为了减少锁的调用，尽量在一次加锁中，释放所有对应该锁(`bin`)的 `region`。

### tcache large
#### 分配
和 `small` 类似，先调用 `tcache_alloc_easy()`，不过若 `tbin` 为空时，不会像 `small` 一样分配所有的 `avail`，而是调用 `arena_malloc_large()` 从 `arena` 中分配一个 `run`。因为创建多个 `large object` 太过昂贵，并且有可能会用不到，浪费空间。

#### 释放
和 `small` 类似，先释放到 `tbin->avail[tbin->ncached]` 中，备用。若该 `tbin` 已满，调用 `arena_bin_flush_large()` 释放一半到 `arena` 中。

### tcache gc
前面注意到，每次分配 `tbin->avail` 中会分配 `ncached_max >> lg_fill_div` 个，若每次均分配固定数目，有可能会造成内存浪费，`jemalloc` 对 `tcache` 中的 `bin` 采用渐进式 `GC`，动态的调整分配数目。有 `2` 个宏控制着 `GC` 的进行:
* `TCACHE_GC_SWEEP`: 可以近似认为每发生该数量的分配或释放操作，所有的 `bin` 都被 `GC`
* `TCACHE_GC_INCR`: 每发生该数量的分配或释放操作，进行一次 `GC`(和 `TCACHE_GC_SWEEP` 有关)

`tcache` 中每个 `bin` 会有如下2个字段:
* `low_water`: 保存着一次 `GC` 时间间隔内，`ncached` 的最小值，也就意味着在这之下的 `avail` 都没被分配
* `lg_fill_div`: 用于控制每次分配的数量(`ncached_max >> lg_fill_div`)，初始为 `1`  

在每次分配和释放时，都会调用 `tcache_event()`，增加 `tcache->ev_cnt`，若和 `TCACHE_GC_INCR` 相等，则调用 `tcache_event_hard()` 对单个 `bin` 进行 `GC`(只对 `small object` 有效)。
`tcache_event_hard()`: 对单个 `bin(next_gc_bin)` 进行 `GC`:
1. 若 `tbin->low_water > 0`: 说明 `tbin->avail` 中有些未被用到，可以尝试减少分配。对应的操作就是释放掉 `3/4 low_water`，`lg_fill_div++`(下次分配时会减少一半)
2. 若 `tbin->low_water < 0`: 只有在该 `tbin->avail` 全部分配完才会置 `low_water = 1`，说明不够用，所以会 `lg_fill_div--`(下次分配时加倍)  
`tcache` 中的 `tbin` 分配数量就会一直动态调整。

## 需要注意如何通过一个 ptr 获取相应的 chunk、bin、run



## purge retained
`dirty` 为已分配之后又被释放的 `run` 或 `chunk`，按照 `page` 维度进行回收。`arena` 中 `runs_dirty` 维护者 `dirty run` 和 `dirty chunk`，而 `chunks_cache` 维护着 `dirty chunk`，应该是处理方式的不同需要区分  

插入到 `cache` 时会增加 `dirty`
`cache` 和 `retain` 的区别：  

```
 MADV_DONTNEED
              Do not expect access in the near future.  (For the time being,
              the application is finished with the given range, so the
              kernel can free resources associated with it.)

              After a successful MADV_DONTNEED operation, the semantics of
              memory access in the specified region are changed: subsequent
              accesses of pages in the range will succeed, but will result
              in either repopulating the memory contents from the up-to-date
              contents of the underlying mapped file (for shared file
              mappings, shared anonymous mappings, and shmem-based
              techniques such as System V shared memory segments) or zero-
              fill-on-demand pages for anonymous private mappings.

              Note that, when applied to shared mappings, MADV_DONTNEED
              might not lead to immediate freeing of the pages in the range.
              The kernel is free to delay freeing the pages until an
              appropriate moment.  The resident set size (RSS) of the
              calling process will be immediately reduced however.

              MADV_DONTNEED cannot be applied to locked pages, Huge TLB
              pages, or VM_PFNMAP pages.  (Pages marked with the kernel-
              internal VM_PFNMAP flag are special memory areas that are not
              managed by the virtual memory subsystem.  Such pages are
              typically created by device drivers that map the pages into
              user space.)
```

## mmap/dss
`dss` 和 `mmap` 的地址不重叠，可因根据地址判断在哪个区。默认使用 `mmap`，`sbrk` 是第二选择。
无论是 brk，还是 mmap，申请的都是虚拟内存，物理内存是由内核在缺页中断时按需分配的。

## 不太清楚的
1. `chunks_cache` 缓存脏 `chunk` ? 怎么来的？哦，释放掉的 `run` 导致整个 `chunk` 为空就到这了应该
2. `cached` 和 `retained` 的区别，分别作用: `cached` 类似 `avail_runs` 的作用，保存空闲的已分配 `chunk`。 `retained` 一般为空，只有在 `chunk_dalloc_arena` 中会插入，一般 `chunk` 会调用 `munmap()` 释放掉，若不支持，则 `purge` 然后插入到 `retained`
3. `node_cache` 中 `node` 怎么来的，`base` 中的 `node` 缓存是用于管理 `base` 分配的空间。`node_cache` 是通过 `base` 分配的，之后释放的会在 `node_cache` 中缓存
4. `base` 和 `chunk_allloc_mmap()` 的作用区别:
  * `base`: 用于 `jemalloc` 内部的分配，比如结构体、`node` 等，也是按照 `chunk` 大小分配，底层的分配还是 `chunk_alloc_mmap()`
  * `chunk_alloc_mmap()`: 分配用于用户使用的空间
5. `chunks_tree` 的作用: `chunk_tree` 保存着 `chunk` 地址到 `extent_node_t` 的映射，保存着所有已经分配出去的 `chunk`，
